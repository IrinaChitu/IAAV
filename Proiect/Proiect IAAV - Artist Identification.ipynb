{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artist Identification using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.initializers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '../input/images/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clean dataset -rename files with weird characters\n",
    "\n",
    "# images = os.listdir(path + \"Albrecht_DuÌˆrer/\")\n",
    "\n",
    "# for idx,img in enumerate(images):\n",
    "#     img_path = path + img\n",
    "#     new_img_name = path + \"Albrecht_Durer_\" + str(idx) + \".jpg\"\n",
    "#     os.rename(img_path, new_img_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "artist_frequency = os.listdir(path)\n",
    "\n",
    "for (index,artist_folder) in enumerate(os.listdir(path)):\n",
    "    artist_frequency[index] = (artist_frequency[index], len( [painting for painting in  os.listdir(path + \"/\" + artist_folder)] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_pos = range(len(artist_frequency))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.barh(x_pos, [ artist[1] for artist in artist_frequency] )\n",
    "plt.title(\"Frequency of Paintigs Per Artist\")\n",
    "\n",
    "plt.yticks(x_pos, [ artist[0].replace(\"_\", \" \") for artist in artist_frequency ])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort artists by number of paintings\n",
    "artist_frequency.sort(key=lambda elem: -elem[1])\n",
    "\n",
    "# Create a dataframe with artists having more than 200 paintings     ## Alternative method: choose maximum 100 pictures of each artist\n",
    "artists_info = [ artist for artist in artist_frequency if artist[1] >= 200 ]\n",
    "nr_total_paintings = sum( nr for (artist, nr) in artists_info)\n",
    "artists_info = [ (artist,nr, nr_total_paintings/(nr*len(artists_info))) for (artist,nr) in artists_info ]\n",
    "\n",
    "artist_classes = [ artist[0] for artist in artists_info ]\n",
    "weights = [ artist[2] for artist in artists_info ]\n",
    "\n",
    "for (idx, artist) in enumerate(artists_info):\n",
    "    print(idx, artist[0].replace(\"_\", \" \"), artist[1], round(artist[2], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_pos = range(len(artists_info))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.barh(x_pos, [ artist[1] for artist in artists_info] )\n",
    "plt.title(\"Frequency of Paintigs Per Artist\")\n",
    "\n",
    "plt.yticks(x_pos, [ artist.replace(\"_\", \" \") for artist in artist_classes])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_w = 2048\n",
    "max_w = 0\n",
    "min_h = 2048\n",
    "max_h = 0\n",
    "\n",
    "images = []\n",
    "\n",
    "for artist_folder in os.listdir(path):\n",
    "    if artist_folder in artist_classes:\n",
    "        for painting in os.listdir(path + artist_folder):\n",
    "            picture_path = path + artist_folder + \"/\" + painting \n",
    "            im = Image.open(picture_path)\n",
    "            images.append(im) # label ish, tho se poate extrage si din im.filename\n",
    "\n",
    "            h, w = im.size\n",
    "            min_h = min(h, min_h)\n",
    "            max_h = max(h, max_h)\n",
    "            min_w = min(w, min_w)\n",
    "            max_w = max(w, max_w)\n",
    "\n",
    "print(\"Picture's dimensions are between:\")\n",
    "print(\"Height: \", min_h, max_h)\n",
    "print(\"Width: \", min_w, max_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random samples from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_label(image): # image: PIL Image\n",
    "    return image.filename.split('/')[3].replace(\"_\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print few random paintings\n",
    "n = 5\n",
    "fig, axes = plt.subplots(1, n, figsize=(20,10))\n",
    "\n",
    "for i in range(n):\n",
    "    random_artist = random.choice(artist_classes)\n",
    "    random_image = random.choice(images)\n",
    "    image = np.asarray(random_image)\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(\"Pictor: \" + random_artist.replace('_', ' '))\n",
    "    axes[i].axis('off') # hide x, y axis\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_crop(x, crop_size=(224,224)):\n",
    "    h, w, _ = x.shape\n",
    "    range_w = (w - crop_size[1])\n",
    "    range_h = (h - crop_size[0])\n",
    "    offset_w = 0 if range_w == 0 else np.random.randint(range_w)\n",
    "    offset_h = 0 if range_h == 0 else np.random.randint(range_h)\n",
    "    cropped_x = x[offset_h:offset_h + crop_size[0], offset_w:offset_w + crop_size[1], :]\n",
    "    return cropped_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def center_crop(x, crop_size=(224,224)):\n",
    "    h, w, _ = x.shape\n",
    "    center_h = h // 2\n",
    "    center_w = w // 2\n",
    "    offset_w = center_h - (crop_size[0] // 2)\n",
    "    offset_h = center_h - (crop_size[0] // 2)\n",
    "    cropped_x = x[offset_h:offset_h + crop_size[0], offset_w:offset_w + crop_size[1], :]\n",
    "    return cropped_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessor(image):\n",
    "    # perform augmentations here\n",
    "    rotate = random.choice([0,1])\n",
    "    if rotate == 0:\n",
    "        aug = random_crop(image)\n",
    "    else:\n",
    "        angle = random.choice( [10*x for x in range(1,36)] )\n",
    "        image = Image.fromarray(image)\n",
    "        img = np.asarray(image.rotate(angle))\n",
    "        aug = center_crop(img)\n",
    "    return aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print a random painting and its customized random augmented version\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "random_artist = random.choice(artist_classes)\n",
    "random_image = random.choice(images)\n",
    "\n",
    "# Original image\n",
    "image = np.asarray(random_image)\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(\"An original Image of \" + random_artist.replace('_', ' '))\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Transformed image\n",
    "aug_image = preprocessor(np.asarray(random_image))\n",
    "axes[1].imshow(aug_image)\n",
    "axes[1].set_title(\"A transformed Image of \" + random_artist.replace('_', ' '))\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Augment data\n",
    "batch_size = 16\n",
    "train_input_shape = (224, 224, 3)\n",
    "n_classes = len(artist_classes)\n",
    "\n",
    "train_datagen = ImageDataGenerator(validation_split=0.2,\n",
    "                                   rescale=1./255., # target values between 0 and 1 by scaling with a 1/255 - recommended\n",
    "                                   #rotation_range=45,\n",
    "                                   #width_shift_range=0.5,\n",
    "                                   #height_shift_range=0.5,\n",
    "                                   shear_range=5,\n",
    "                                   #zoom_range=0.7,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "#                                    preprocessing_function=preprocessor\n",
    "                                  )\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory=path,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=train_input_shape[0:2],\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    subset=\"training\",\n",
    "                                                    shuffle=True,\n",
    "                                                    classes=artist_classes\n",
    "                                                   )\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(directory=path,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=train_input_shape[0:2],\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    subset=\"validation\",\n",
    "                                                    shuffle=True,\n",
    "                                                    classes=artist_classes\n",
    "                                                   )\n",
    "\n",
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "print(\"Total number of batches =\", STEP_SIZE_TRAIN, \"and\", STEP_SIZE_VALID) # Train for 215 steps, validate for 53 steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformed pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print a random painting and its random augmented version\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "random_artist = random.choice(artist_classes)\n",
    "random_image = random.choice(os.listdir(os.path.join(path, random_artist)))\n",
    "random_image_file = os.path.join(path, random_artist, random_image)\n",
    "\n",
    "# Original image\n",
    "image = plt.imread(random_image_file)\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(\"An original image of \" + random_artist.replace('_', ' '))\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Transformed image\n",
    "aug_image = train_datagen.random_transform(image)\n",
    "axes[1].imshow(aug_image)\n",
    "axes[1].set_title(\"A transformed image of \" + random_artist.replace('_', ' '))\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE Convulsional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=train_input_shape)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add layers at the end\n",
    "X = base_model.output\n",
    "X = Flatten()(X)\n",
    "\n",
    "X = Dense(512, kernel_initializer='he_uniform')(X) # params: units:int pozitiv, dimensiunea outputului | kernel_initializer - intitializatorul pt kernel (weights matrix) - distributie uniforma intr un interval pe baza de formule\n",
    "#X = Dropout(0.5)(X) # takes in a float between 0 and 1, which is the fraction of the neurons to drop # helps prevent overfitting\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "\n",
    "X = Dense(16, kernel_initializer='he_uniform')(X)\n",
    "#X = Dropout(0.5)(X)\n",
    "X = BatchNormalization()(X)\n",
    "X = Activation('relu')(X)\n",
    "\n",
    "output = Dense(n_classes, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = Adam(lr=0.0001) # stochastic gradient descent method\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, \n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epoch = 10\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, \n",
    "                           mode='auto', restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, # if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced\n",
    "                              verbose=1, mode='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model - all layers\n",
    "history1 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=n_epoch,\n",
    "                              shuffle=True,\n",
    "                              verbose=1,\n",
    "                              callbacks=[reduce_lr],\n",
    "#                               use_multiprocessing=True,\n",
    "                              workers=16,\n",
    "                              class_weight=weights\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During transfer learning the first layers of the network are frozen while leaving the end layers open to modification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Freeze core ResNet layers and train again \n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model.layers[:50]:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = Adam(lr=0.0001)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "n_epoch = 50\n",
    "history2 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,\n",
    "                              epochs=n_epoch,\n",
    "                              shuffle=True,\n",
    "                              verbose=1,\n",
    "                              callbacks=[reduce_lr, early_stop],\n",
    "#                               use_multiprocessing=True,\n",
    "                              workers=16,\n",
    "                              class_weight=weights\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss value implies how poorly or well a model behaves after each iteration of optimization.\n",
    "\n",
    "# Merge history1 and history2\n",
    "history = {}\n",
    "# history['loss'] = history1.history['loss'] + history2.history['loss']\n",
    "# history['acc'] = history1.history['acc'] + history2.history['acc']\n",
    "# history['val_loss'] = history1.history['val_loss'] + history2.history['val_loss']\n",
    "# history['val_acc'] = history1.history['val_acc'] + history2.history['val_acc']\n",
    "# history['lr'] = history1.history['lr'] + history2.history['lr']\n",
    "\n",
    "# Fara a doua antrenare\n",
    "history['loss'] = history1.history['loss']\n",
    "history['acc'] = history1.history['accuracy']\n",
    "history['val_loss'] = history1.history['val_loss']\n",
    "history['val_acc'] = history1.history['val_accuracy']\n",
    "history['lr'] = history1.history['lr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the training graph\n",
    "def plot_training(history):\n",
    "    acc = history['acc']\n",
    "    val_acc = history['val_acc']\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "    \n",
    "    axes[0].plot(epochs, acc, 'r-', label='Training Accuracy')\n",
    "    axes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')\n",
    "    axes[0].set_title('Training and Validation Accuracy')\n",
    "    axes[0].legend(loc='best')\n",
    "\n",
    "    axes[1].plot(epochs, loss, 'r-', label='Training Loss')\n",
    "    axes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')\n",
    "    axes[1].set_title('Training and Validation Loss')\n",
    "    axes[1].legend(loc='best')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_training(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction accuracy on train data\n",
    "score = model.evaluate_generator(train_generator, verbose=1)\n",
    "print(\"Prediction accuracy on train data =\", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction accuracy on CV data\n",
    "score = model.evaluate_generator(valid_generator, verbose=1)\n",
    "print(\"Prediction accuracy on validation data =\", score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import *\n",
    "import seaborn as sns\n",
    "\n",
    "tick_labels = artist_classes\n",
    "\n",
    "def showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID):\n",
    "    # Loop on each generator batch and predict\n",
    "    y_pred, y_true = [], []\n",
    "    for i in range(STEP_SIZE_VALID):\n",
    "        (X,y) = next(valid_generator)\n",
    "        y_pred.append(model.predict(X))\n",
    "        y_true.append(y)\n",
    "    \n",
    "    # Create a flat list for y_true and y_pred\n",
    "    y_pred = [subresult for result in y_pred for subresult in result]\n",
    "    y_true = [subresult for result in y_true for subresult in result]\n",
    "    \n",
    "    # Update Truth vector based on argmax\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    \n",
    "    # Update Prediction vector based on argmax\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=np.arange(n_classes))\n",
    "    conf_matrix = conf_matrix/np.sum(conf_matrix, axis=1)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\".2f\", square=True, cbar=False, \n",
    "                cmap=plt.cm.jet, xticklabels=tick_labels, yticklabels=tick_labels,\n",
    "                ax=ax)\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_true, y_pred, labels=np.arange(n_classes), target_names=artist_classes))\n",
    "\n",
    "showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prediction\n",
    "from keras.preprocessing import *\n",
    "\n",
    "n = 5\n",
    "fig, axes = plt.subplots(1, n, figsize=(25,10))\n",
    "\n",
    "for i in range(n):\n",
    "    random_artist = random.choice(artist_classes)\n",
    "    random_image = random.choice(os.listdir(os.path.join(path, random_artist)))\n",
    "    random_image_file = os.path.join(path, random_artist, random_image)\n",
    "\n",
    "    # Original image\n",
    "    test_image = image.load_img(random_image_file, target_size=(train_input_shape[0:2]))\n",
    "\n",
    "    # Predict artist\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image /= 255.\n",
    "    test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "    prediction = model.predict(test_image)\n",
    "    prediction_probability = np.amax(prediction)\n",
    "    prediction_idx = np.argmax(prediction)\n",
    "\n",
    "    labels = train_generator.class_indices\n",
    "    labels = dict((v,k) for k,v in labels.items())\n",
    "\n",
    "    title = \"Actual artist = {}\\nPredicted artist = {}\\nPrediction probability = {:.2f} %\" \\\n",
    "                .format(random_artist.replace('_', ' '), labels[prediction_idx].replace('_', ' '),\n",
    "                        prediction_probability*100)\n",
    "\n",
    "    # Print image\n",
    "    axes[i].imshow(plt.imread(random_image_file))\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
